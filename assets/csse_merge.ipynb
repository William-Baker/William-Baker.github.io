{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /home/w/Documents/MSDV/venv/lib/python3.9/site-packages (1.4.2)\n","Requirement already satisfied: pytz>=2020.1 in /home/w/Documents/MSDV/venv/lib/python3.9/site-packages (from pandas) (2022.1)\n","Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /home/w/Documents/MSDV/venv/lib/python3.9/site-packages (from pandas) (1.22.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /home/w/Documents/MSDV/venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /home/w/Documents/MSDV/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: tqdm in /home/w/Documents/MSDV/venv/lib/python3.9/site-packages (4.64.0)\n"]}],"source":["\n","#%%\n","!python -m pip install pandas\n","!python -m pip install tqdm\n","\n","import pandas as pd\n","from tqdm import tqdm\n","tqdm.pandas()\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","\n","path = \"\"\"/home/w/Documents/MSDV/COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/\"\"\"\n","\n","from distutils.command.clean import clean\n","from os import listdir\n","from os.path import isfile, join\n","\n","# Get all the file names for all the CSV files\n","files = [f for f in listdir(path) if isfile(join(path, f))]\n","csvs = [f for f in files if \".csv\" in f]\n","\n","# Read all the CSV files and merge them into a single large pandas data frame\n","dfs = [pd.read_csv(path + c) for c in csvs]\n","df = pd.concat(dfs)\n","\n","\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2952726/2952726 [00:45<00:00, 65386.20it/s]\n","100%|██████████| 2952726/2952726 [00:45<00:00, 64819.77it/s]\n","100%|██████████| 2952726/2952726 [00:40<00:00, 72085.14it/s]\n","100%|██████████| 2952726/2952726 [00:40<00:00, 72507.00it/s]\n","100%|██████████| 2952726/2952726 [00:46<00:00, 63568.13it/s]\n","100%|██████████| 2952726/2952726 [00:55<00:00, 53233.61it/s]\n","100%|██████████| 2952726/2952726 [00:55<00:00, 53355.39it/s]\n"]}],"source":["\n","# There are duplicated columns, in this section, we 'unify' these duplicates taking whichever attribute is not NA\n","def unify(e):\n","    ret = pd.NA\n","    for col in e.index:\n","        if e[col] != \"<NA>\" and not pd.isna(e[col]):\n","            return e[col]\n","    return pd.NA\n","\n","clean_df = pd.DataFrame()\n","clean_df['Province'] = df[['Province/State', 'Province_State']].progress_apply(unify, axis=1)\n","clean_df['Country'] = df[['Country/Region', 'Country_Region']].progress_apply(unify, axis=1)\n","clean_df['lat'] = df[['Lat', 'Latitude']].progress_apply(unify, axis=1)\n","clean_df['lng'] = df[['Long_', 'Longitude']].progress_apply(unify, axis=1)\n","clean_df['Last Update'] = df[['Last Update', 'Last_Update']].progress_apply(unify, axis=1)\n","clean_df['Case Fatality Ratio'] = df[['Case-Fatality_Ratio', 'Case_Fatality_Ratio']].progress_apply(unify, axis=1)\n","clean_df['Incidence Rate'] = df[['Incidence_Rate','Incident_Rate']].progress_apply(unify, axis=1)\n","\n","common = ['Confirmed', 'Deaths', 'Recovered', 'FIPS', 'Admin2',  'Active', 'Combined_Key']\n","clean_df[common] = df[common]\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["\n","\n","clean_df.dropna(subset=['lat', 'lng'], inplace=True)\n","# To increase performance and normalise the density of the heatmap, round coordinates to the nearest integer\n","clean_df['lat'] = clean_df['lat'].astype(int)\n","clean_df['lng'] = clean_df['lng'].astype(int)\n","\n","# Fill any NA values with 0, so all frequencies are numeric\n","clean_df[['Confirmed', 'Deaths', 'Recovered', 'Active']] = clean_df[['Confirmed', 'Deaths', 'Recovered', 'Active']].fillna(0)\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4087/4087 [00:08<00:00, 481.19it/s]\n"]}],"source":["\n","# function to de-accumulate the cumulative sum over the whole dataset, so we can view each datapoint in isolation\n","def df_sort_decumulate(df):\n","    df = pd.DataFrame(df.sort_values('Last Update', ascending=True))\n","    for col in ['Confirmed', 'Deaths', 'Recovered', 'Active']:\n","        l = df[col].to_list()\n","        n = [0] * len(l)\n","        for i in range(1, len(l)):\n","            n[i] = l[i] - l[i-1]\n","        df[col] = n\n","    return df\n","\n","# The data has been accumulated over time, this isnt ideal for the time series graph\n","grouped = clean_df.groupby('Combined_Key').progress_apply(lambda df: df_sort_decumulate(df)) # The function is applied to each table of grouped values, \n","        # then pandas automatically concatenates the result, back into one big table like what we started with\n","\n","# ensure the date attribute is correctly interpreted\n","grouped['Last Update'] = pd.to_datetime(grouped['Last Update'])\n","\n","for col in ['Confirmed', 'Deaths', 'Recovered', 'Active']:\n","    grouped[col] = grouped[col].apply(lambda x: 0 if x < 0 else x)\n",""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","# Group by location and date, on a weekly frequency\n","monthly_freq = grouped.groupby(['lat', 'lng', pd.Grouper(key='Last Update', freq='7D')]).sum()\n","monthly_freq = monthly_freq.reset_index()\n",""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","# Define functions to export the data into a JSON, using correct ISO date formatting\n","def series_to_json(series):\n","    if series.dtype == '<M8[ns]':\n","        lst = str(series.apply(lambda x: x.isoformat()).to_list())\n","        lst = lst.replace('\\'', '\\\"')\n","    else:\n","        lst = str(series.to_list())\n","    return f\"\\\"{series.name}\\\": {lst}\"\n","\n","def df_to_json(df):\n","    json_str = \"{\"\n","    for col in df.columns:\n","        json_str += \"\\n\" + series_to_json(df[col])\n","        json_str += \",\"\n","    json_str = json_str.removesuffix(',')  \n","    json_str += \"\\n}\"\n","    return json_str\n","\n","# Index the data by location and export each location to JSON\n","json_by_lat_lng = monthly_freq.groupby(['lat', 'lng']).apply(lambda df: df_to_json(df[['Last Update', 'Confirmed', 'Deaths', 'Recovered', 'Active']]))\n",""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["\n","# Reset the index so we can access it\n","json_by_lat_lng_df = pd.DataFrame(json_by_lat_lng, columns=['data'])\n","json_by_lat_lng_df = json_by_lat_lng_df.reset_index()\n","\n","# Iterate through each location and add the location data to the json - unify each location into a single JSON file\n","json_str = \"[\"\n","for index, row in json_by_lat_lng_df.iterrows():\n","    json_str += \"{\\n\" f\"\\\"lat\\\": {row['lat']},\\n\\\"lng\\\": {row['lng']},\\n\\\"data\\\": {row['data']}\" + \"},\\n\"\n","json_str = json_str.removesuffix(\",\\n\")\n","json_str += \"\\n]\"\n",""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","with open(\"global_covid.json\", \"w\") as text_file:\n","    text_file.write(json_str)\n","text_file.close()"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}